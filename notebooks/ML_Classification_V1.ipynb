{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LED_KO1jQw2-"
      },
      "outputs": [],
      "source": [
        "# # # Credit Card Default Prediction - Google Colab Version\n",
        "# # ## Machine Learning Project with Google Drive Integration\n",
        "# # ### 1. Mount Google Drive and Install Packages\n",
        "# # %%\n",
        "# # Mount Google Drive to access your files\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Navigate to your project folder (update the path as needed)\n",
        "# import os\n",
        "# project_path = '/content/drive/MyDrive'  # Update this to your actual path\n",
        "# os.chdir(project_path)# ### 13. Project Summary\n",
        "# # %%\n",
        "# print(\"\\n\" + \"=\"*60)\n",
        "# print(\"PROJECT SUMMARY - CREDIT CARD DEFAULT PREDICTION\")\n",
        "# print(\"=\"*60)\n",
        "\n",
        "# print(f\"\\nüìÅ Dataset: credit_card_default.csv\")\n",
        "# print(f\"üìä Total samples: {len(df_clean):,}\")\n",
        "# print(f\"üéØ Target variable: {target_col}\")\n",
        "# print(f\"üîß Features used: {X.shape[1]}\")\n",
        "\n",
        "# print(f\"\\nü§ñ Models trained: {', '.join(models.keys())}\")\n",
        "# print(f\"‚öñÔ∏è  Class balancing: {'SMOTE' if USE_SMOTE else 'class_weight parameter'}\")\n",
        "\n",
        "# print(f\"\\nüèÜ Best performing model: {best_model_name}\")\n",
        "# print(f\"   üìà F1-Score: {best_result['f1']:.4f}\")\n",
        "# print(f\"   üéØ Accuracy: {best_result['accuracy']:.4f}\")\n",
        "\n",
        "# print(f\"\\n‚úÖ Preprocessing steps completed:\")\n",
        "# print(\"   1. Google Drive mounting and file loading\")\n",
        "# print(\"   2. Missing values check and handling\")\n",
        "# print(\"   3. Duplicate removal\")\n",
        "# print(\"   4. Column name standardization\")\n",
        "# print(\"   5. Target variable identification\")\n",
        "# print(\"   6. Feature scaling\")\n",
        "# print(\"   7. Train-test split with stratification\")\n",
        "# print(\"   8. Class imbalance handling\")\n",
        "# print(\"   9. Model training and evaluation\")\n",
        "\n",
        "# print(f\"\\nüéØ Key insights:\")\n",
        "# print(f\"   ‚Ä¢ Default rate: {(target_counts[1]/total*100):.1f}%\")\n",
        "# print(f\"   ‚Ä¢ Class imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
        "# print(f\"   ‚Ä¢ Best model achieves {best_result['f1']:.1%} F1-Score\")\n",
        "\n",
        "# print(\"\\n\" + \"=\"*60)\n",
        "# print(\"PROJECT COMPLETED SUCCESSFULLY! ‚úÖ\")\n",
        "# print(\"=\"*60)\n",
        "# print(f\"Current working directory: {os.getcwd()}\")\n",
        "\n",
        "# # List files to verify\n",
        "# print(\"\\nFiles in directory:\")\n",
        "# print(os.listdir('.'))\n",
        "\n",
        "# # Install compatible package versions\n",
        "# !pip install scikit-learn==1.5.0 imbalanced-learn==0.13.0 -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 2. Import Libraries with Compatibility Check\n",
        "# %%\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                           f1_score, confusion_matrix, classification_report, roc_auc_score)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check versions\n",
        "import sklearn\n",
        "print(f\"‚úÖ scikit-learn version: {sklearn.__version__}\")\n",
        "\n",
        "# Try to import SMOTE, fallback to alternative if fails\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    print(\"‚úÖ SMOTE imported successfully\")\n",
        "    USE_SMOTE = True\n",
        "except ImportError as e:\n",
        "    print(f\"‚ö†Ô∏è SMOTE import failed: {e}\")\n",
        "    print(\"Using sklearn class_weight balancing instead\")\n",
        "    USE_SMOTE = False"
      ],
      "metadata": {
        "id": "NqWwclp9Rldu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 3. Load Dataset from Google Drive\n",
        "# %%\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"LOADING DATASET FROM GOOGLE DRIVE\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Load the dataset\n",
        "try:\n",
        "    # Try to load from current directory\n",
        "    df = pd.read_csv('credit_card_default.csv')\n",
        "    print(\"‚úÖ Dataset loaded from 'credit_card_default.csv'\")\n",
        "except FileNotFoundError:\n",
        "    # If not found, search in common locations\n",
        "    print(\"Searching for file in Google Drive...\")\n",
        "\n",
        "    # Common paths to check\n",
        "    search_paths = [\n",
        "        '/content/drive/MyDrive/',\n",
        "        '/content/drive/MyDrive/ML_Project/',\n",
        "        '/content/drive/MyDrive/ML_Project/data/',\n",
        "        '/content/drive/MyDrive/data/'\n",
        "    ]\n",
        "\n",
        "    file_found = False\n",
        "    for path in search_paths:\n",
        "        try:\n",
        "            file_path = os.path.join(path, 'credit_card_default.csv')\n",
        "            if os.path.exists(file_path):\n",
        "                df = pd.read_csv(file_path)\n",
        "                print(f\"‚úÖ Dataset loaded from: {file_path}\")\n",
        "                file_found = True\n",
        "                break\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    if not file_found:\n",
        "        # If still not found, upload directly\n",
        "        print(\"File not found. Please upload the CSV file.\")\n",
        "        from google.colab import files\n",
        "        uploaded = files.upload()\n",
        "        for filename in uploaded.keys():\n",
        "            df = pd.read_csv(filename)\n",
        "            print(f\"‚úÖ Dataset loaded from uploaded file: {filename}\")\n",
        "\n",
        "print(f\"\\nDataset Shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "display(df.head())\n"
      ],
      "metadata": {
        "id": "ePlI_vaqRwZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 4. Data Preprocessing and Cleaning\n",
        "# %%\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DATA PREPROCESSING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create a clean copy\n",
        "df_clean = df.copy()\n",
        "\n",
        "# 1. Check for missing values\n",
        "print(\"\\n1. MISSING VALUES CHECK:\")\n",
        "missing_values = df_clean.isnull().sum()\n",
        "missing_percentage = (missing_values / len(df_clean)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing_Count': missing_values,\n",
        "    'Percentage': missing_percentage\n",
        "})\n",
        "display(missing_df[missing_df['Missing_Count'] > 0])\n",
        "\n",
        "if missing_values.sum() == 0:\n",
        "    print(\"‚úÖ No missing values found\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Found {missing_values.sum()} missing values\")\n",
        "    # Fill numeric missing values with median\n",
        "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
        "    df_clean[numeric_cols] = df_clean[numeric_cols].fillna(df_clean[numeric_cols].median())\n",
        "    print(\"Filled numeric missing values with median\")\n",
        "\n",
        "# 2. Check for duplicates\n",
        "print(\"\\n2. DUPLICATES CHECK:\")\n",
        "duplicate_rows = df_clean.duplicated().sum()\n",
        "print(f\"Duplicate rows: {duplicate_rows}\")\n",
        "if duplicate_rows > 0:\n",
        "    df_clean = df_clean.drop_duplicates()\n",
        "    print(f\"‚úÖ Removed {duplicate_rows} duplicate rows\")\n",
        "else:\n",
        "    print(\"‚úÖ No duplicate rows found\")\n",
        "\n",
        "# 3. Check column names and standardize\n",
        "print(\"\\n3. COLUMN NAMES STANDARDIZATION:\")\n",
        "print(f\"Original columns: {df_clean.columns.tolist()}\")\n",
        "\n",
        "# Standardize column names (remove spaces, dots, convert to lowercase)\n",
        "df_clean.columns = df_clean.columns.str.strip().str.lower().str.replace('.', '_').str.replace(' ', '_')\n",
        "print(f\"Standardized columns: {df_clean.columns.tolist()}\")\n",
        "\n",
        "# Identify target column\n",
        "possible_targets = ['default_payment_next_month', 'default_next_month', 'default', 'y']\n",
        "target_col = None\n",
        "for col in possible_targets:\n",
        "    if col in df_clean.columns:\n",
        "        target_col = col\n",
        "        break\n",
        "\n",
        "if target_col is None:\n",
        "    # Try to find any binary column that could be the target\n",
        "    binary_cols = []\n",
        "    for col in df_clean.columns:\n",
        "        unique_vals = df_clean[col].nunique()\n",
        "        if unique_vals == 2 and df_clean[col].dtype in [np.int64, np.float64, int, float]:\n",
        "            binary_cols.append(col)\n",
        "\n",
        "    if binary_cols:\n",
        "        target_col = binary_cols[0]\n",
        "        print(f\"Assuming target column is: {target_col} (binary column)\")\n",
        "    else:\n",
        "        raise ValueError(\"Could not identify target column. Please check your dataset.\")\n",
        "\n",
        "print(f\"‚úÖ Target column identified: {target_col}\")"
      ],
      "metadata": {
        "id": "gjYn0FWQR3_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Drop unnecessary columns\n",
        "cols_to_drop = []\n",
        "if 'id' in df_clean.columns:\n",
        "    cols_to_drop.append('id')\n",
        "if 'customer_id' in df_clean.columns:\n",
        "    cols_to_drop.append('customer_id')\n",
        "\n",
        "if cols_to_drop:\n",
        "    df_clean = df_clean.drop(columns=cols_to_drop)\n",
        "    print(f\"‚úÖ Dropped unnecessary columns: {cols_to_drop}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Final dataset shape: {df_clean.shape}\")\n",
        "print(f\"‚úÖ Features: {len(df_clean.columns) - 1}\")\n",
        "print(f\"‚úÖ Samples: {len(df_clean)}\")"
      ],
      "metadata": {
        "id": "0nuIyvasSBc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 5. Target Variable Analysis\n",
        "# %%\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TARGET VARIABLE ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "target_counts = df_clean[target_col].value_counts()\n",
        "total = len(df_clean)\n",
        "\n",
        "print(f\"Class 0 (No Default): {target_counts[0]:,} ({(target_counts[0]/total*100):.2f}%)\")\n",
        "if len(target_counts) > 1:\n",
        "    print(f\"Class 1 (Default): {target_counts[1]:,} ({(target_counts[1]/total*100):.2f}%)\")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars = plt.bar(['No Default', 'Default'], target_counts.values,\n",
        "               color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
        "plt.title('Credit Card Default Distribution', fontsize=14)\n",
        "plt.ylabel('Number of Clients')\n",
        "plt.xlabel('Default Status')\n",
        "\n",
        "# Add count labels on bars\n",
        "for bar, count in zip(bars, target_counts.values):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100,\n",
        "             f'{count:,}\\n({count/total*100:.1f}%)',\n",
        "             ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Check for class imbalance\n",
        "imbalance_ratio = target_counts.max() / target_counts.min()\n",
        "if imbalance_ratio > 1.5:\n",
        "    print(f\"‚ö†Ô∏è Class imbalance detected: Ratio = {imbalance_ratio:.2f}:1\")\n",
        "    print(\"Will apply class balancing techniques\")\n",
        "else:\n",
        "    print(\"‚úÖ Classes are relatively balanced\")"
      ],
      "metadata": {
        "id": "57FJlLbaSQ-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 6. Data Preparation and Feature Engineering\n",
        "# %%\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FEATURE PREPARATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Separate features and target\n",
        "X = df_clean.drop(columns=[target_col])\n",
        "y = df_clean[target_col]\n",
        "\n",
        "print(f\"Features (X) shape: {X.shape}\")\n",
        "print(f\"Target (y) shape: {y.shape}\")\n",
        "\n",
        "# Check feature types\n",
        "print(\"\\nFeature types:\")\n",
        "print(X.dtypes.value_counts())\n",
        "\n",
        "# Feature scaling\n",
        "print(\"\\nApplying feature scaling...\")\n",
        "scaler = StandardScaler()\n",
        "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "print(\"‚úÖ Feature scaling completed\")"
      ],
      "metadata": {
        "id": "6bia6OBfSQ8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 7. Train-Test Split\n",
        "# %%\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAIN-TEST SPLIT\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
        "print(f\"\\nTraining class distribution:\")\n",
        "print(f\"  Class 0: {sum(y_train == 0):,} ({(sum(y_train == 0)/len(y_train)*100):.1f}%)\")\n",
        "print(f\"  Class 1: {sum(y_train == 1):,} ({(sum(y_train == 1)/len(y_train)*100):.1f}%)\")"
      ],
      "metadata": {
        "id": "12Nrzb-_SbHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 8. Handle Class Imbalance\n",
        "# %%\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"HANDLING CLASS IMBALANCE\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if USE_SMOTE:\n",
        "    print(\"Using SMOTE for class balancing...\")\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    print(f\"\\nAfter SMOTE:\")\n",
        "    print(f\"  Training samples: {X_train_bal.shape[0]:,}\")\n",
        "    print(f\"  Class 0: {sum(y_train_bal == 0):,}\")\n",
        "    print(f\"  Class 1: {sum(y_train_bal == 1):,}\")\n",
        "else:\n",
        "    print(\"Using class_weight parameter in models (no resampling)\")\n",
        "    X_train_bal, y_train_bal = X_train, y_train\n"
      ],
      "metadata": {
        "id": "luUWv8BmScZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 9. Model Training with Fast Algorithms\n",
        "# %%\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL TRAINING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Initialize results dictionary\n",
        "results = {}\n"
      ],
      "metadata": {
        "id": "_g8wVc1ITpGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 9.1 Logistic Regression Model\n",
        "# %%\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"MODEL 1: LOGISTIC REGRESSION\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Define model based on SMOTE availability\n",
        "if USE_SMOTE:\n",
        "    lr_model = LogisticRegression(random_state=42, max_iter=1000, C=1.0)\n",
        "else:\n",
        "    lr_model = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000, C=1.0)\n",
        "\n",
        "# Train model\n",
        "print(\"Training Logistic Regression...\")\n",
        "lr_model.fit(X_train_bal, y_train_bal)\n",
        "print(\"‚úì Training completed\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "y_pred_proba_lr = lr_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "precision_lr = precision_score(y_test, y_pred_lr)\n",
        "recall_lr = recall_score(y_test, y_pred_lr)\n",
        "f1_lr = f1_score(y_test, y_pred_lr)\n",
        "\n",
        "# Store results\n",
        "results['Logistic Regression'] = {\n",
        "    'model': lr_model,\n",
        "    'y_pred': y_pred_lr,\n",
        "    'y_pred_proba': y_pred_proba_lr,\n",
        "    'accuracy': accuracy_lr,\n",
        "    'precision': precision_lr,\n",
        "    'recall': recall_lr,\n",
        "    'f1': f1_lr\n",
        "}\n",
        "\n",
        "# Display results\n",
        "print(\"\\nüìä Logistic Regression Performance:\")\n",
        "print(f\"  Accuracy:  {accuracy_lr:.4f}\")\n",
        "print(f\"  Precision: {precision_lr:.4f}\")\n",
        "print(f\"  Recall:    {recall_lr:.4f}\")\n",
        "print(f\"  F1-Score:  {f1_lr:.4f}\")\n",
        "\n",
        "# Display coefficients if available\n",
        "if hasattr(lr_model, 'coef_'):\n",
        "    print(\"\\nüîç Top 10 Feature Coefficients (Absolute Value):\")\n",
        "    coef_df = pd.DataFrame({\n",
        "        'Feature': X.columns,\n",
        "        'Coefficient': lr_model.coef_[0],\n",
        "        'Abs_Coefficient': np.abs(lr_model.coef_[0])\n",
        "    }).sort_values('Abs_Coefficient', ascending=False).head(10)\n",
        "    display(coef_df[['Feature', 'Coefficient']])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O_6GdzSBTHdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 9.2 Decision Tree Model\n",
        "# %%\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"MODEL 2: DECISION TREE\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Define model based on SMOTE availability\n",
        "if USE_SMOTE:\n",
        "    dt_model = DecisionTreeClassifier(random_state=42, max_depth=5, min_samples_split=10)\n",
        "else:\n",
        "    dt_model = DecisionTreeClassifier(class_weight='balanced', random_state=42, max_depth=5, min_samples_split=10)\n",
        "\n",
        "# Train model\n",
        "print(\"Training Decision Tree...\")\n",
        "dt_model.fit(X_train_bal, y_train_bal)\n",
        "print(\"‚úì Training completed\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "y_pred_proba_dt = dt_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "precision_dt = precision_score(y_test, y_pred_dt)\n",
        "recall_dt = recall_score(y_test, y_pred_dt)\n",
        "f1_dt = f1_score(y_test, y_pred_dt)\n",
        "\n",
        "# Store results\n",
        "results['Decision Tree'] = {\n",
        "    'model': dt_model,\n",
        "    'y_pred': y_pred_dt,\n",
        "    'y_pred_proba': y_pred_proba_dt,\n",
        "    'accuracy': accuracy_dt,\n",
        "    'precision': precision_dt,\n",
        "    'recall': recall_dt,\n",
        "    'f1': f1_dt\n",
        "}\n",
        "\n",
        "# Display results\n",
        "print(\"\\nüìä Decision Tree Performance:\")\n",
        "print(f\"  Accuracy:  {accuracy_dt:.4f}\")\n",
        "print(f\"  Precision: {precision_dt:.4f}\")\n",
        "print(f\"  Recall:    {recall_dt:.4f}\")\n",
        "print(f\"  F1-Score:  {f1_dt:.4f}\")\n",
        "\n",
        "# Display feature importance\n",
        "if hasattr(dt_model, 'feature_importances_'):\n",
        "    print(\"\\nüîç Top 10 Feature Importances:\")\n",
        "    feature_importance_dt = pd.DataFrame({\n",
        "        'Feature': X.columns,\n",
        "        'Importance': dt_model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False).head(10)\n",
        "    display(feature_importance_dt)\n",
        "\n",
        "# Visualize tree structure\n",
        "print(\"\\nüå≥ Tree Depth Information:\")\n",
        "print(f\"  Tree Depth: {dt_model.get_depth()}\")\n",
        "print(f\"  Number of Leaves: {dt_model.get_n_leaves()}\")"
      ],
      "metadata": {
        "id": "ABXvxcpCTWcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 9.3 Random Forest Model\n",
        "# %%\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"MODEL 3: RANDOM FOREST\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Define model based on SMOTE availability\n",
        "if USE_SMOTE:\n",
        "    rf_model = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10, n_jobs=-1)\n",
        "else:\n",
        "    rf_model = RandomForestClassifier(class_weight='balanced', random_state=42, n_estimators=100, max_depth=10, n_jobs=-1)\n",
        "\n",
        "# Train model\n",
        "print(\"Training Random Forest...\")\n",
        "rf_model.fit(X_train_bal, y_train_bal)\n",
        "print(\"‚úì Training completed\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "precision_rf = precision_score(y_test, y_pred_rf)\n",
        "recall_rf = recall_score(y_test, y_pred_rf)\n",
        "f1_rf = f1_score(y_test, y_pred_rf)\n",
        "\n",
        "# Store results\n",
        "results['Random Forest'] = {\n",
        "    'model': rf_model,\n",
        "    'y_pred': y_pred_rf,\n",
        "    'y_pred_proba': y_pred_proba_rf,\n",
        "    'accuracy': accuracy_rf,\n",
        "    'precision': precision_rf,\n",
        "    'recall': recall_rf,\n",
        "    'f1': f1_rf\n",
        "}\n",
        "\n",
        "# Display results\n",
        "print(\"\\nüìä Random Forest Performance:\")\n",
        "print(f\"  Accuracy:  {accuracy_rf:.4f}\")\n",
        "print(f\"  Precision: {precision_rf:.4f}\")\n",
        "print(f\"  Recall:    {recall_rf:.4f}\")\n",
        "print(f\"  F1-Score:  {f1_rf:.4f}\")\n",
        "\n",
        "# Display feature importance\n",
        "if hasattr(rf_model, 'feature_importances_'):\n",
        "    print(\"\\nüîç Top 10 Feature Importances:\")\n",
        "    feature_importance_rf = pd.DataFrame({\n",
        "        'Feature': X.columns,\n",
        "        'Importance': rf_model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False).head(10)\n",
        "    display(feature_importance_rf)\n",
        "\n",
        "# Forest information\n",
        "print(\"\\nüå≤ Random Forest Information:\")\n",
        "print(f\"  Number of Trees: {len(rf_model.estimators_)}\")\n",
        "print(f\"  Tree Depth Range: {min([tree.get_depth() for tree in rf_model.estimators_])}-{max([tree.get_depth() for tree in rf_model.estimators_])}\")"
      ],
      "metadata": {
        "id": "5szOVd5JTkFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 9.4 Training Summary\n",
        "# %%\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(f\"\\n‚úÖ All {len(results)} models trained successfully!\")\n",
        "print(f\"Class balancing method: {'SMOTE' if USE_SMOTE else 'class_weight parameter'}\")\n",
        "\n",
        "# Create summary table\n",
        "summary_data = []\n",
        "for name, result in results.items():\n",
        "    summary_data.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': result['accuracy'],\n",
        "        'Precision': result['precision'],\n",
        "        'Recall': result['recall'],\n",
        "        'F1-Score': result['f1']\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "summary_df = summary_df.sort_values('F1-Score', ascending=False)\n",
        "\n",
        "print(\"\\nüìã Performance Summary (Sorted by F1-Score):\")\n",
        "print(\"-\" * 70)\n",
        "display(summary_df.style.format({\n",
        "    'Accuracy': '{:.4f}',\n",
        "    'Precision': '{:.4f}',\n",
        "    'Recall': '{:.4f}',\n",
        "    'F1-Score': '{:.4f}'\n",
        "}).background_gradient(cmap='YlOrRd', subset=['F1-Score']))\n",
        "\n",
        "# Identify best model\n",
        "best_model_name = summary_df.iloc[0]['Model']\n",
        "best_result = results[best_model_name]\n",
        "\n",
        "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
        "print(f\"   F1-Score: {best_result['f1']:.4f}\")\n",
        "print(f\"   Accuracy: {best_result['accuracy']:.4f}\")\n",
        "\n",
        "print(\"\\nüìä Training Set Information:\")\n",
        "print(f\"  Original size: {X_train.shape[0]:,} samples\")\n",
        "print(f\"  After balancing: {X_train_bal.shape[0]:,} samples\")\n",
        "print(f\"  Test set size: {X_test.shape[0]:,} samples\")"
      ],
      "metadata": {
        "id": "cQnloxHpTdnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# ### 10. Model Comparison\n",
        "# %%\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create comparison dataframe\n",
        "comparison_data = []\n",
        "for name, result in results.items():\n",
        "    comparison_data.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': result['accuracy'],\n",
        "        'Precision': result['precision'],\n",
        "        'Recall': result['recall'],\n",
        "        'F1-Score': result['f1']\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df = comparison_df.sort_values('F1-Score', ascending=False)\n",
        "\n",
        "print(\"\\nüìä Model Performance Summary:\")\n",
        "print(\"-\" * 60)\n",
        "display(comparison_df.style.format({\n",
        "    'Accuracy': '{:.4f}',\n",
        "    'Precision': '{:.4f}',\n",
        "    'Recall': '{:.4f}',\n",
        "    'F1-Score': '{:.4f}'\n",
        "}).background_gradient(cmap='YlOrRd', subset=['F1-Score', 'Accuracy']))\n",
        "\n",
        "# Visual comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "x = np.arange(len(results))  # CHANGED: Use len(results) instead of len(models)\n",
        "width = 0.2\n",
        "\n",
        "models_list = comparison_df['Model'].tolist()\n",
        "\n",
        "for i, metric in enumerate(['Accuracy', 'Precision', 'Recall', 'F1-Score']):\n",
        "    offset = width * (i - 1.5)\n",
        "    values = comparison_df[metric].values\n",
        "    plt.bar(x + offset, values, width, label=metric, alpha=0.8)\n",
        "\n",
        "plt.xlabel('Models', fontsize=12)\n",
        "plt.ylabel('Score', fontsize=12)\n",
        "plt.title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
        "plt.xticks(x, models_list)\n",
        "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1))\n",
        "plt.ylim([0, 1])\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ydL5ONAfT2VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 11. Confusion Matrices (ESSENTIAL)\n",
        "# %%\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"CONFUSION MATRICES\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "fig.suptitle('Confusion Matrices for Credit Card Default Prediction', fontsize=14, fontweight='bold')\n",
        "\n",
        "for idx, (name, result) in enumerate(results.items()):\n",
        "    cm = confusion_matrix(y_test, result['y_pred'])\n",
        "\n",
        "    # Create heatmap\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                ax=axes[idx], cbar=False,\n",
        "                annot_kws={'size': 12, 'weight': 'bold'})\n",
        "\n",
        "    axes[idx].set_title(f'{name}\\nF1: {result[\"f1\"]:.3f}', fontsize=12)\n",
        "    axes[idx].set_xlabel('Predicted Label', fontsize=10)\n",
        "    axes[idx].set_ylabel('True Label', fontsize=10)\n",
        "    axes[idx].set_xticklabels(['No Default', 'Default'])\n",
        "    axes[idx].set_yticklabels(['No Default', 'Default'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cOUnobOsT7vA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 11. Enhanced Confusion Matrices with Metrics Display\n",
        "# %%\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ENHANCED CONFUSION MATRICES WITH METRICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create a 2x2 grid for better visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Enhanced Confusion Matrices with Performance Metrics', fontsize=16, fontweight='bold')\n",
        "\n",
        "models_list = list(results.keys())\n",
        "\n",
        "# First row: Standard confusion matrices with metrics on top\n",
        "for idx, name in enumerate(models_list):\n",
        "    result = results[name]\n",
        "    cm = confusion_matrix(y_test, result['y_pred'])\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    # Create enhanced title with metrics\n",
        "    metrics_title = f'{name}\\nAcc: {result[\"accuracy\"]:.3f} | F1: {result[\"f1\"]:.3f} | Prec: {result[\"precision\"]:.3f} | Rec: {result[\"recall\"]:.3f}'\n",
        "\n",
        "    # Create heatmap with annotations\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                ax=axes[0, idx], cbar=True,\n",
        "                annot_kws={'size': 14, 'weight': 'bold', 'color': 'white' if cm.max() > cm.sum()/2 else 'black'},\n",
        "                linewidths=1, linecolor='gray')\n",
        "\n",
        "    axes[0, idx].set_title(metrics_title, fontsize=12, fontweight='bold', pad=12)\n",
        "    axes[0, idx].set_xlabel('Predicted Label', fontsize=11)\n",
        "    axes[0, idx].set_ylabel('True Label', fontsize=11)\n",
        "    axes[0, idx].set_xticklabels(['No Default\\n(0)', 'Default\\n(1)'], fontsize=10)\n",
        "    axes[0, idx].set_yticklabels(['No Default\\n(0)', 'Default\\n(1)'], fontsize=10, rotation=0)\n",
        "\n",
        "    # Add additional text annotations inside the plot\n",
        "    axes[0, idx].text(0.5, -0.15, f'TN={tn} | FP={fp} | FN={fn} | TP={tp}',\n",
        "                      transform=axes[0, idx].transAxes, ha='center', fontsize=10,\n",
        "                      bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "\n",
        "# ### 11.1 Single Large Confusion Matrix with All Metrics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPREHENSIVE CONFUSION MATRIX WITH ALL METRICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create one large figure for the best model\n",
        "best_result = results[best_model_name]\n",
        "best_cm = confusion_matrix(y_test, best_result['y_pred'])\n",
        "tn, fp, fn, tp = best_cm.ravel()\n",
        "total = best_cm.sum()\n",
        "\n",
        "fig = plt.figure(figsize=(14, 10))\n",
        "\n",
        "# Create grid for layout\n",
        "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# Main confusion matrix (span 2x2)\n",
        "ax1 = fig.add_subplot(gs[0:2, 0:2])\n",
        "\n",
        "# Create the confusion matrix heatmap\n",
        "sns.heatmap(best_cm, annot=True, fmt='d', cmap='Blues',\n",
        "            ax=ax1, cbar=False,\n",
        "            annot_kws={'size': 18, 'weight': 'bold', 'color': 'white'},\n",
        "            linewidths=2, linecolor='black')\n",
        "\n",
        "ax1.set_title(f'{best_model_name} - Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\n",
        "ax1.set_xlabel('Predicted Label', fontsize=14)\n",
        "ax1.set_ylabel('True Label', fontsize=14)\n",
        "ax1.set_xticklabels(['No Default\\n(Class 0)', 'Default\\n(Class 1)'], fontsize=12)\n",
        "ax1.set_yticklabels(['No Default\\n(Class 0)', 'Default\\n(Class 1)'], fontsize=12, rotation=0)\n",
        "\n",
        "# Add cell annotations with percentages\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        value = best_cm[i, j]\n",
        "        percentage = value / total * 100\n",
        "        ax1.text(j + 0.5, i + 0.3, f'{value}\\n({percentage:.1f}%)',\n",
        "                ha='center', va='center', fontsize=11,\n",
        "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
        "\n",
        "# Metrics panel (right side)\n",
        "ax2 = fig.add_subplot(gs[0:2, 2])\n",
        "\n",
        "# Hide axes for text display\n",
        "ax2.axis('off')\n",
        "\n",
        "# Create metrics display\n",
        "metrics_text = f\"\"\"\n",
        "{best_model_name} - PERFORMANCE METRICS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "CONFUSION MATRIX VALUES:\n",
        "‚Ä¢ True Negatives (TN): {tn:,}\n",
        "‚Ä¢ False Positives (FP): {fp:,}\n",
        "‚Ä¢ False Negatives (FN): {fn:,}\n",
        "‚Ä¢ True Positives (TP): {tp:,}\n",
        "‚Ä¢ Total Samples: {total:,}\n",
        "\n",
        "PRIMARY METRICS:\n",
        "‚Ä¢ Accuracy:   {best_result['accuracy']:.4f}\n",
        "‚Ä¢ Precision:  {best_result['precision']:.4f}\n",
        "‚Ä¢ Recall:     {best_result['recall']:.4f}\n",
        "‚Ä¢ F1-Score:   {best_result['f1']:.4f}\n",
        "\n",
        "DERIVED METRICS:\n",
        "‚Ä¢ Specificity: {tn/(tn+fp):.4f}\n",
        "‚Ä¢ NPV:        {tn/(tn+fn):.4f}\n",
        "‚Ä¢ FPR:        {fp/(fp+tn):.4f}\n",
        "‚Ä¢ FNR:        {fn/(fn+tp):.4f}\n",
        "\n",
        "ERROR ANALYSIS:\n",
        "‚Ä¢ Type I Errors:  {fp} ({fp/total*100:.1f}%)\n",
        "‚Ä¢ Type II Errors: {fn} ({fn/total*100:.1f}%)\n",
        "‚Ä¢ Correct:       {tp+tn} ({(tp+tn)/total*100:.1f}%)\n",
        "\"\"\"\n",
        "\n",
        "ax2.text(0.1, 0.95, metrics_text, fontsize=12, family='monospace',\n",
        "         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
        "\n",
        "# Bottom panel: Metric comparison visualization\n",
        "ax3 = fig.add_subplot(gs[2, :])\n",
        "\n",
        "# Prepare data for bar chart\n",
        "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "values_to_plot = [best_result['accuracy'], best_result['precision'],\n",
        "                  best_result['recall'], best_result['f1']]\n",
        "\n",
        "colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
        "bars = ax3.bar(metrics_to_plot, values_to_plot, color=colors, alpha=0.8, edgecolor='black')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, value in zip(bars, values_to_plot):\n",
        "    height = bar.get_height()\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2, height + 0.01,\n",
        "            f'{value:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "ax3.set_ylabel('Score', fontsize=12)\n",
        "ax3.set_title(f'{best_model_name} - Key Metrics Visualization', fontsize=14, fontweight='bold')\n",
        "ax3.set_ylim([0, 1])\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "ax3.set_axisbelow(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xFG_8Eg7ZhFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 11.2 All Models Side-by-Side with Metrics\n",
        "# %%\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ALL MODELS - SIDE-BY-SIDE COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "fig, axes = plt.subplots(1, len(results), figsize=(16, 5))\n",
        "fig.suptitle('Model Comparison: Confusion Matrices with Key Metrics', fontsize=16, fontweight='bold', y=1.05)\n",
        "\n",
        "for idx, (name, result) in enumerate(results.items()):\n",
        "    cm = confusion_matrix(y_test, result['y_pred'])\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    # Create the heatmap\n",
        "    im = axes[idx].imshow(cm, cmap='YlOrRd', interpolation='nearest', vmin=0, vmax=cm.max())\n",
        "\n",
        "    # Add text annotations for each cell\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            value = cm[i, j]\n",
        "            percentage = value / cm.sum() * 100\n",
        "            text_color = 'white' if value > cm.max()/2 else 'black'\n",
        "            axes[idx].text(j, i, f'{value}\\n({percentage:.1f}%)',\n",
        "                          ha='center', va='center',\n",
        "                          color=text_color, fontsize=11, fontweight='bold')\n",
        "\n",
        "    # Set labels\n",
        "    axes[idx].set(xticks=[0, 1], yticks=[0, 1],\n",
        "                  xticklabels=['No Default', 'Default'],\n",
        "                  yticklabels=['No Default', 'Default'])\n",
        "    axes[idx].set_xlabel('Predicted', fontsize=11)\n",
        "    axes[idx].set_ylabel('Actual', fontsize=11)\n",
        "\n",
        "    # Add metrics as title\n",
        "    title_text = f'{name}\\nAcc: {result[\"accuracy\"]:.3f} | F1: {result[\"f1\"]:.3f}'\n",
        "    axes[idx].set_title(title_text, fontsize=12, fontweight='bold', pad=10)\n",
        "\n",
        "    # Add detailed metrics below\n",
        "    metrics_text = f'Prec: {result[\"precision\"]:.3f}\\nRec: {result[\"recall\"]:.3f}'\n",
        "    axes[idx].text(0.5, -0.25, metrics_text, transform=axes[idx].transAxes,\n",
        "                   ha='center', fontsize=10,\n",
        "                   bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Add colorbar\n",
        "cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
        "fig.colorbar(im, cax=cbar_ax)\n",
        "cbar_ax.set_ylabel('Count', fontsize=11)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xBRJOwUHZbQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 11.3 Metrics Summary Table\n",
        "# %%\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPREHENSIVE METRICS SUMMARY TABLE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create detailed metrics table\n",
        "detailed_metrics = []\n",
        "for name, result in results.items():\n",
        "    cm = confusion_matrix(y_test, result['y_pred'])\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    total = cm.sum()\n",
        "\n",
        "    # Calculate additional metrics\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
        "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "    balanced_acc = (result['recall'] + specificity) / 2\n",
        "\n",
        "    detailed_metrics.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': result['accuracy'],\n",
        "        'Precision': result['precision'],\n",
        "        'Recall': result['recall'],\n",
        "        'F1-Score': result['f1'],\n",
        "        'Specificity': specificity,\n",
        "        'NPV': npv,\n",
        "        'Balanced Acc': balanced_acc,\n",
        "        'FPR': fpr,\n",
        "        'FNR': fnr,\n",
        "        'TP': tp,\n",
        "        'TN': tn,\n",
        "        'FP': fp,\n",
        "        'FN': fn\n",
        "    })\n",
        "\n",
        "detailed_df = pd.DataFrame(detailed_metrics)\n",
        "detailed_df = detailed_df.sort_values('F1-Score', ascending=False)\n",
        "\n",
        "print(\"\\nüìä Complete Metrics Summary:\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "# Format the display\n",
        "styled_df = detailed_df.style.format({\n",
        "    'Accuracy': '{:.4f}',\n",
        "    'Precision': '{:.4f}',\n",
        "    'Recall': '{:.4f}',\n",
        "    'F1-Score': '{:.4f}',\n",
        "    'Specificity': '{:.4f}',\n",
        "    'NPV': '{:.4f}',\n",
        "    'Balanced Acc': '{:.4f}',\n",
        "    'FPR': '{:.4f}',\n",
        "    'FNR': '{:.4f}',\n",
        "    'TP': '{:d}',\n",
        "    'TN': '{:d}',\n",
        "    'FP': '{:d}',\n",
        "    'FN': '{:d}'\n",
        "})\n",
        "\n",
        "# Apply gradient to key metrics\n",
        "styled_df = styled_df.background_gradient(cmap='YlOrRd', subset=['Accuracy', 'F1-Score', 'Balanced Acc'])\n",
        "\n",
        "display(styled_df)\n",
        "\n",
        "# Print key insights\n",
        "print(f\"\\nüí° KEY INSIGHTS FROM CONFUSION MATRICES:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"1. Best Model: {best_model_name} (F1-Score: {results[best_model_name]['f1']:.4f})\")\n",
        "print(f\"2. Highest Accuracy: {detailed_df['Accuracy'].max():.4f} ({detailed_df.loc[detailed_df['Accuracy'].idxmax()]['Model']})\")\n",
        "print(f\"3. Highest Precision: {detailed_df['Precision'].max():.4f} ({detailed_df.loc[detailed_df['Precision'].idxmax()]['Model']})\")\n",
        "print(f\"4. Highest Recall: {detailed_df['Recall'].max():.4f} ({detailed_df.loc[detailed_df['Recall'].idxmax()]['Model']})\")\n",
        "print(f\"5. Most Balanced: {detailed_df['Balanced Acc'].max():.4f} ({detailed_df.loc[detailed_df['Balanced Acc'].idxmax()]['Model']})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"METRICS DISPLAY ENHANCEMENT COMPLETE\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "gwURGIYOZQuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 12. Best Model Analysis\n",
        "# %%\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"BEST MODEL ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Identify best model\n",
        "best_model_name = comparison_df.iloc[0]['Model']\n",
        "best_result = results[best_model_name]\n",
        "best_model = best_result['model']\n",
        "\n",
        "print(f\"üèÜ BEST MODEL: {best_model_name}\")\n",
        "print(f\"   F1-Score: {best_result['f1']:.4f}\")\n",
        "print(f\"   Accuracy: {best_result['accuracy']:.4f}\")\n",
        "print(f\"   Precision: {best_result['precision']:.4f}\")\n",
        "print(f\"   Recall: {best_result['recall']:.4f}\")\n",
        "\n",
        "print(\"\\nüìã Classification Report:\")\n",
        "print(\"-\" * 50)\n",
        "print(classification_report(y_test, best_result['y_pred'],\n",
        "                           target_names=['No Default', 'Default']))\n",
        "\n",
        "# Feature importance (if available)\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    print(\"\\nüîç Top 10 Feature Importances:\")\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': X.columns,\n",
        "        'Importance': best_model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False).head(10)\n",
        "\n",
        "    display(feature_importance)\n",
        "\n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(feature_importance['Feature'], feature_importance['Importance'],\n",
        "             color='teal', alpha=0.7)\n",
        "    plt.xlabel('Importance Score', fontsize=12)\n",
        "    plt.title(f'Top 10 Feature Importances - {best_model_name}', fontsize=14)\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.grid(True, alpha=0.3, axis='x')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Zm3mYDlfUBRY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}